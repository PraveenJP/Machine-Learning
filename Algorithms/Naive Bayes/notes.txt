Naive Bayes

What is Naive Bayes?
    It is a classification technique based on Bayesâ€™ theorem with an assumption of independence between predictors. 
    In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. 
    For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. 
    Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.

    P(c|x) = P(X|c)P(c)/P(x)

    Here,
    P(c|x) is the posterior probability of class (target) given predictor (attribute). 
    P(c) is the prior probability of class. 
    P(x|c) is the likelihood which is the probability of predictor given class. 
    P(x) is the prior probability of predictor.

Python Code 
==============

#Import Library
from sklearn.naive_bayes import GaussianNB
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)